{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"fully_connected.ipynb","provenance":[],"authorship_tag":"ABX9TyOCD3W+PDxkkXbmpCKIUdC/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"DZ2m7iDOpo2u"},"source":["import numpy as np\n","import pickle"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Lpq8oFZMpusg"},"source":["from os import path, makedirs, remove\n","\n","from utilities.initializers import he_normal\n","from utilities.settings import get_layer_num, inc_layer_num\n","\n","np.random.seed(0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rWTtXmDRpthe"},"source":["class FullyConnected:\n","    def __init__(self, units=200, name=None):\n","        self.units = units\n","        self.params = {}\n","        self.cache = {}\n","        self.grads = {}\n","        self.momentum_cache = {}\n","        self.rmsprop_cache = {}\n","        self.has_units = True\n","        self.type = 'fc'\n","        self.name = name\n","\n","    def has_weights(self):\n","        return self.has_units\n","\n","    def save_weights(self, dump_path):\n","        dump_cache = {\n","            'cache': self.cache,\n","            'grads': self.grads,\n","            'momentum': self.momentum_cache,\n","            'rmsprop_cache': self.rmsprop_cache\n","        }\n","        save_path = path.join(dump_path, self.name+'.pickle')\n","        makedirs(path.dirname(save_path), exist_ok=True)\n","        remove(save_path)\n","        with open(save_path, 'wb') as d:\n","            pickle.dump(dump_cache, d)\n","\n","    def load_weights(self, dump_path):\n","        if self.name is None:\n","            self.name = '{}_{}'.format(self.type, get_layer_num(self.type))\n","            inc_layer_num(self.type)\n","        read_path = path.join(dump_path, self.name+'.pickle')\n","        with open(read_path, 'rb') as r:\n","            dump_cache = pickle.load(r)\n","        self.cache = dump_cache['cache']\n","        self.grads = dump_cache['grads']\n","        self.momentum_cache = dump_cache['momentum']\n","        self.rmsprop_cache = dump_cache['rmsprop_cache']\n","\n","    def forward_propagate(self, X, save_cache=False):\n","        if self.name is None:\n","            self.name = '{}_{}'.format(self.type, get_layer_num(self.type))\n","            inc_layer_num(self.type)\n","\n","        if 'W' not in self.params:\n","            self.params['W'], self.params['b'] = he_normal((X.shape[0], self.units))\n","        Z = np.dot(self.params['W'], X) + self.params['b']\n","        if save_cache:\n","            self.cache['A'] = X\n","        return Z\n","\n","    def back_propagate(self, dZ):\n","        batch_size = dZ.shape[1]\n","        self.grads['dW'] = np.dot(dZ, self.cache['A'].T) / batch_size\n","        self.grads['db'] = np.sum(dZ, axis=1, keepdims=True)\n","        return np.dot(self.params['W'].T, dZ)\n","\n","    def init_cache(self):\n","        cache = dict()\n","        cache['dW'] = np.zeros_like(self.params['W'])\n","        cache['db'] = np.zeros_like(self.params['b'])\n","        return cache\n","\n","    def momentum(self, beta=0.9):\n","        if not self.momentum_cache:\n","            self.momentum_cache = self.init_cache()\n","        self.momentum_cache['dW'] = beta * self.momentum_cache['dW'] + (1 - beta) * self.grads['dW']\n","        self.momentum_cache['db'] = beta * self.momentum_cache['db'] + (1 - beta) * self.grads['db']\n","\n","    def rmsprop(self, beta=0.999, amsprop=True):\n","        if not self.rmsprop_cache:\n","            self.rmsprop_cache = self.init_cache()\n","\n","        new_dW = beta * self.rmsprop_cache['dW'] + (1 - beta) * (self.grads['dW']**2)\n","        new_db = beta * self.rmsprop_cache['db'] + (1 - beta) * (self.grads['db']**2)\n","\n","        if amsprop:\n","            self.rmsprop_cache['dW'] = np.maximum(self.rmsprop_cache['dW'], new_dW)\n","            self.rmsprop_cache['db'] = np.maximum(self.rmsprop_cache['db'], new_db)\n","        else:\n","            self.rmsprop_cache['dW'] = new_dW\n","            self.rmsprop_cache['db'] = new_db\n","\n","    def apply_grads(self, learning_rate=0.001, l2_penalty=1e-4, optimization='adam', epsilon=1e-8, \\\n","                    correct_bias=False, beta1=0.9, beta2=0.999, iter=999):\n","        if optimization != 'adam':\n","            self.params['W'] -= learning_rate * (self.grads['dW'] + l2_penalty * self.params['W'])\n","            self.params['b'] -= learning_rate * (self.grads['db'] + l2_penalty * self.params['b'])\n","        else:\n","            if correct_bias:\n","                W_first_moment = self.momentum_cache['dW'] / (1 - beta1 ** iter)\n","                b_first_moment = self.momentum_cache['db'] / (1 - beta1 ** iter)\n","                W_second_moment = self.rmsprop_cache['dW'] / (1 - beta2 ** iter)\n","                b_second_moment = self.rmsprop_cache['db'] / (1 - beta2 ** iter)\n","            else:\n","                W_first_moment = self.momentum_cache['dW']\n","                b_first_moment = self.momentum_cache['db']\n","                W_second_moment = self.rmsprop_cache['dW']\n","                b_second_moment = self.rmsprop_cache['db']\n","\n","            W_learning_rate = learning_rate / (np.sqrt(W_second_moment) + epsilon)\n","            b_learning_rate = learning_rate / (np.sqrt(b_second_moment) + epsilon)\n","\n","            self.params['W'] -= W_learning_rate * (W_first_moment + l2_penalty * self.params['W'])\n","            self.params['b'] -= b_learning_rate * (b_first_moment + l2_penalty * self.params['b'])"],"execution_count":null,"outputs":[]}]}